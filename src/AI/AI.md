AI（Artificial Intelligence，人工智能）

AGI（Artificial General Intelligence，通用人工智能）



# LLM（Large Language Model，大语言模型）

大语言模型是一种人工智能模型，旨在理解和生成人类语言。它们在大量的文本数据上进行训练，可以执行广泛的任务，包括文本总结、翻译、情感分析等等。LLM的特点是**规模庞大，包含数十亿的参数**，帮助它们学习语言数据中的复杂模式。这些模型通常基于深度学习架构，如转化器，这有助于它们在各种NLP任务上取得令人印象深刻的表现

从量变到质变，当数据量超过某个临界点时，模型实现了显著的性能提升，并出现了小模型中不存在的能力，比如上下文学习（in-context learning）。

这也就催生了两个事件：

1. 各大AI巨头提高训练参数量以期达到更好的效果
2. 由于质变原因的无法解释带来的AI安全性考量

## LLM应用方案

### RAG（Retrieval Augmented Generation，检索增强生成）

原LLM的缺陷：

- 知识的局限性：模型自身的知识完全源于它的训练数据，而现有的主流大模型（ChatGPT、文心一言、通义千问…）的训练集基本都是构建于网络公开的数据，对于一些实时性的、非公开的或离线的数据是无法获取到的，这部分知识也就无从具备
- 幻觉问题：所有的AI模型的底层原理都是基于数学概率，其模型输出实质上是一系列数值运算，大模型也不例外，所以它有时候会一本正经地胡说八道，尤其是在大模型自身不具备某一方面的知识或不擅长的场景。而这种幻觉问题的区分是比较困难的，因为它要求使用者自身具备相应领域的知识
- 数据安全性：对于企业来说，数据安全至关重要，没有企业愿意承担数据泄露的风险，将自身的私域数据上传第三方平台进行训练。这也导致完全依赖通用大模型自身能力的应用方案不得不在数据安全和效果方面进行取舍

RAG是解决上述问题的一套有效方案，RAG的架构如图中所示，简单来讲，RAG就是通过检索获取相关的知识并将其融入Prompt，让大模型能够参考相应的知识从而给出合理回答。因此，可以将RAG的核心理解为“检索+生成”，前者主要是利用向量数据库的高效存储和检索能力，召回目标知识；后者则是利用大模型和Prompt工程，将召回的知识合理利用，生成目标答案

![img](https://raw.githubusercontent.com/GIT-GAZZ/typora-cloud-image/master/image/v2-17970c2b4f7613d74276234eaba59e81_720w-813d8de4763b8cd1298638ee77675b48.webp)

完整的RAG应用流程主要包含两个阶段：

- 数据准备阶段：数据提取——>文本分割——>向量化（embedding）——>数据入库
- 应用阶段：用户提问——>数据检索（召回）——>注入Prompt——>LLM生成答案



# LLaMA（Large Language Model Meta AI）